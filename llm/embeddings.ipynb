{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "- Creates a lookup table of the provided width and height\n",
    "    - Width being the number of vectors and height being the size of each vector\n",
    "- Embedding tables can be queried using `id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "embedding_table = nn.Embedding(10, 4)\n",
    "embedding_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding table can be preloaded with values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4, 4, 5]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor([[1, 2, 3, 4], [2, 4, 4, 5], [5, 6, 7, 8], [3, 5, 6,7]])\n",
    "preloaded_embed_table = nn.Embedding.from_pretrained(weights)\n",
    "\n",
    "preloaded_embed_table.embedding_dim\n",
    "preloaded_embed_table(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.FloatTensor([[1, 2, 3, 4], [2, 4, 4, 5], [5, 6, 7, 8], [3, 5, 6,7]])\n",
    "preloaded_embed_table = nn.Embedding.from_pretrained(weights, max_norm=5.48)\n",
    "\n",
    "preloaded_embed_table.embedding_dim\n",
    "preloaded_embed_table(torch.tensor([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm of vectors\n",
    "- Pytorch defines the norm of a vector as its length\n",
    "- Let $V = ai + bj + ck$\n",
    "- Norm of a vector is calculated as\n",
    "$$\n",
    "\\left | \\left | V \\right | \\right | =\n",
    "\\sqrt{a^2 + b^2 + c^2}\n",
    "$$\n",
    "\n",
    "### `max_norm`\n",
    "- When `max_norm` is specified, if the normal of a vector in the embedding table is larger than `max_norm`, then that vector is renormalised.\n",
    "- Formula for normalisation\n",
    "$$\n",
    "\\text{normal} (V) = \\frac{V}{||V||}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1.9974, 3.9948, 3.9948, 4.9934]])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.FloatTensor([[1, 2, 3, 4], [2, 4, 4, 5], [5, 6, 7, 8], [3, 5, 6,7]])\n",
    "preloaded_embed_table = nn.Embedding.from_pretrained(weights, max_norm=7.8)\n",
    "\n",
    "print(preloaded_embed_table(torch.tensor([0])))\n",
    "print(preloaded_embed_table(torch.tensor([1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mlxt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
