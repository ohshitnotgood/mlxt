{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention masks\n",
    "_Source: [What are attention masks](https://lukesalamone.github.io/posts/what-are-attention-masks/)_\n",
    "\n",
    "- Predictions in a GPT can be made in parallel.\n",
    "- However, sending in parallel requests needs all the inputs to be of the same length.\n",
    "    - The same length meaning, they need to contain the same number of tokens.\n",
    "    - `It will rain in the` and `My dog is` have different token sizes\n",
    "- The solution is to add padding.\n",
    "- Attention mask tells a transformer which tokens are padding tokens.\n",
    "- Padding is added to the right side by default but can be added to the left side as well.\n",
    "- The `Tokenizer` needs a padding token. \n",
    "- Huggingface documentation defines `pad_token` as:\n",
    "    > A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by attention mechanisms or loss computation. Will be associated to `self.pad_token` and `self.pad_token_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1026,   481,  6290,   287,   262],\n",
       "        [50256, 50256,  3666,  3290,   318]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokeniser.padding_side = \"left\"\n",
    "tokeniser.pad_token = tokeniser.eos_token\n",
    "\n",
    "\n",
    "sentences = [\"It will rain in the\", \"My dog is\"]\n",
    "\n",
    "tokeniser(sentences, return_tensors=\"pt\", padding=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mlxt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
