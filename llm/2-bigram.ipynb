{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Bigram model\n",
    "\n",
    "- An NLP model that uses the current token to predict the next token.\n",
    "- Essentially, calculating the probability of the next word, given the current word\n",
    "$$\n",
    "P(\\text{next word} | \\text{current word})\n",
    "$$\n",
    "- This uses the Markov's assumption that, the probability of the next word given the current word is the same as the probaility of the next word given the previous sequence of words.\n",
    "    - So if we have a sentence _The quick Scribe jumped over the lazy Paladin_, we want to find $P(\\text{Paladin} | \\text{lazy})$.\n",
    "    - We make the assumption that $P(\\text{Paladin | lazy}) \\approx P(\\text{Paladin | The quick Scribe jumped over the lazy})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Karpathy's Bigram model\n",
    "- In his video about GPTs, Andrej Karpathy uses a lookup table to create the Bigram model\n",
    "- He uses `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further exploration\n",
    "### Smoothing\n",
    "#### Laplace smoothing"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
